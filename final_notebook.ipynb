{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Predicting-Covid-Cases-From-Google-Search-Data:-A-Time-Series-Analysis\" data-toc-modified-id=\"Predicting-Covid-Cases-From-Google-Search-Data:-A-Time-Series-Analysis-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Predicting Covid Cases From Google Search Data: A Time Series Analysis</a></span></li><li><span><a href=\"#Business-and-Data-Understanding\" data-toc-modified-id=\"Business-and-Data-Understanding-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Business and Data Understanding</a></span><ul class=\"toc-item\"><li><span><a href=\"#Business-Problem\" data-toc-modified-id=\"Business-Problem-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Business Problem</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Limitations-of-the-dataset\" data-toc-modified-id=\"Limitations-of-the-dataset-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Limitations of the dataset</a></span></li><li><span><a href=\"#Why-We-Used-This-Dataset\" data-toc-modified-id=\"Why-We-Used-This-Dataset-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Why We Used This Dataset</a></span></li><li><span><a href=\"#Dataset-Size\" data-toc-modified-id=\"Dataset-Size-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Dataset Size</a></span></li></ul></li></ul></li><li><span><a href=\"#Initial-Look-at-the-Data\" data-toc-modified-id=\"Initial-Look-at-the-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Initial Look at the Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Google-Search-Data\" data-toc-modified-id=\"Google-Search-Data-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Google Search Data</a></span></li><li><span><a href=\"#Public-COVID-19-PA-Data\" data-toc-modified-id=\"Public-COVID-19-PA-Data-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Public COVID-19 PA Data</a></span></li><li><span><a href=\"#Joining-the-Google-and-Public-COVID-19-Data\" data-toc-modified-id=\"Joining-the-Google-and-Public-COVID-19-Data-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Joining the Google and Public COVID-19 Data</a></span></li></ul></li><li><span><a href=\"#Initial-Visualizations\" data-toc-modified-id=\"Initial-Visualizations-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Initial Visualizations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-Train-Split:-Target-Variable\" data-toc-modified-id=\"Test-Train-Split:-Target-Variable-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Test Train Split: Target Variable</a></span></li></ul></li></ul></li><li><span><a href=\"#Base-Model-of-the-Target-Variable\" data-toc-modified-id=\"Base-Model-of-the-Target-Variable-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Base Model of the Target Variable</a></span></li><li><span><a href=\"#Multivariate-Time-Series-Model\" data-toc-modified-id=\"Multivariate-Time-Series-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Multivariate Time Series Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#PCA\" data-toc-modified-id=\"PCA-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>PCA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Granger-Causality-Test\" data-toc-modified-id=\"Granger-Causality-Test-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Granger Causality Test</a></span></li></ul></li><li><span><a href=\"#VAR-model\" data-toc-modified-id=\"VAR-model-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>VAR model</a></span></li></ul></li><li><span><a href=\"#Final-Model-Evaluation\" data-toc-modified-id=\"Final-Model-Evaluation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Final Model Evaluation</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Conclusion</a></span><ul class=\"toc-item\"><li><span><a href=\"#Summary-of-Analysis\" data-toc-modified-id=\"Summary-of-Analysis-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Summary of Analysis</a></span></li><li><span><a href=\"#Recommendations\" data-toc-modified-id=\"Recommendations-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Recommendations</a></span></li><li><span><a href=\"#Next-Steps\" data-toc-modified-id=\"Next-Steps-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Next Steps</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Covid Cases From Google Search Data: A Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business and Data Understanding\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "The city of Philadelphia has been tracking its COVID-19 cases since the beginning of the pandemic. They want to know if they can use Google's COVID-19 related search data to better predict COVID-19 rates. If a model to give advanced warning could be made it would give the city of Philadelphia time to be able to prepare for spikes in COVID-19 cases and increase preventative measures (mask mandates, encouraging social distancing, warn hospitals, etc.) when needed.\n",
    "\n",
    "## Data\n",
    "\n",
    "In order to do this analysis, there were two data sets that we put together. \n",
    "\n",
    "The independent variables were taken from [Google's Explore COVID-19 Symptoms Search Trends](https://pair-code.github.io/covid19_symptom_dataset/?country=IE). The data was downloaded from the USA region (sub region of Pennsylvania) at the daily resolution. All of the data from January 1st, 2020 through November 11th, 2022 was then collated into one data frame, containing 68,805 rows and 430 columns. This data had all been [scaled and normalized](https://storage.googleapis.com/gcp-public-data-symptom-search/COVID-19%20Search%20Trends%20symptoms%20dataset%20documentation%20.pdf) prior to being downloaded. \n",
    "\n",
    "The target variable was taken from [COVID-19 Data for Pennsylvania](https://www.health.pa.gov/topics/disease/coronavirus/pages/Cases.aspx). This data spanned from March 1st, 2020 until March 14, 2023 and included 75,412 rows and 12 columns. \n",
    "\n",
    "### Limitations of the dataset\n",
    "\n",
    "While these two datasets do include a fairly comprehensive list of the search terms and COVID-19 case counts, they do not include all of the possible elements relevant to the rise and fall of COVID-19 cases - for example they don't take into account the proliferation of novel versions of the virus (e.g. Delta, Omicron, etc.). The dataset is also limited by time - [COVID-19 was only proclaimed a pandemic by the World Health Organization on March 11th, 2020](https://www.yalemedicine.org/news/covid-timeline). As such, it could be that better predictions will be available as more time passes, allowing for more data to be collected. \n",
    "\n",
    "### Why We Used This Dataset\n",
    "\n",
    "As the initial inquiry was about if search trends could be used to predict COVID-19, we felt that these data sets were a perfect place to start! Google is a leading search engine, so it seemed an intuitive place to collect search data from. All of the acquired data was free and publicly available. \n",
    "\n",
    "### Dataset Size\n",
    "\n",
    "Initially, we had two datasets, one with **68,805** rows and **430** columns, the other with **75,412** rows and **12** columns. After subsetting these datasets to include **only the Philadelphia region**, cleaning the data, and matching the dates of the datasets, we had **991** rows (representing from March 8th, 2020 to November 13th, 2022) and **423** columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Look at the Data\n",
    " \n",
    " \n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from scipy.stats import pearsonr\n",
    "import statsmodels.api as sm\n",
    "import pmdarima as pm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import log\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, adfuller\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bringing in our 4 dataframes\n",
    "\n",
    "public_data = pd.read_csv(\n",
    "    \"data/COVID-19_Aggregate_Cases_Current_Weekly_County_Health.csv\")\n",
    "google_data_2020 = pd.read_csv(\n",
    "    \"data/2020_sub_region_1_daily_2020_US_Pennsylvania_daily_symptoms_dataset.csv\",  dtype=str)\n",
    "google_data_2021 = pd.read_csv(\n",
    "    \"data/2021_sub_region_1_daily_2021_US_Pennsylvania_daily_symptoms_dataset(1).csv\", dtype=str)\n",
    "google_data_2022 = pd.read_csv(\n",
    "    \"data/2022_sub_region_1_daily_2022_US_Pennsylvania_daily_symptoms_dataset(1).csv\", dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Google Search Data\n",
    "\n",
    "Let's start by consolidating all of our separate Google files into one data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidating the google data into one dataframe\n",
    "google_dataframes = [google_data_2020, google_data_2021, google_data_2022]\n",
    "google_data = pd.concat(google_dataframes)\n",
    "google_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the size and makeup of our dataframe\n",
    "google_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's extract *only* the data relevant to Philadelphia and clean up our dataset a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_philly = google_data[google_data.sub_region_2 == 'Philadelphia County']\n",
    "google_philly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing extraneous columns from our `google_data`\n",
    "google_philly.drop(columns=[\"sub_region_2\", \"sub_region_2_code\", \"country_region_code\", \"country_region\", \"sub_region_1\",\n",
    "                            \"sub_region_1_code\", \"place_id\"], inplace=True)\n",
    "\n",
    "google_philly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the date our index, then sort by the date index\n",
    "google_philly['date'] = pd.to_datetime(google_philly['date'])\n",
    "google_philly.set_index('date', inplace=True)\n",
    "google_philly = google_philly.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_philly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing all our variable types from objects to float\n",
    "google_philly = google_philly.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/52044348/check-for-any-missing-dates-in-the-index\n",
    "\n",
    "# checking for any missing dates!\n",
    "pd.date_range(start='2020-01-01',\n",
    "              end='2022-11-13').difference(google_philly.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values in `google_philly`\n",
    "google_philly.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for 0 in the whole data frame - we don't find any, so lets use 0 as the new value for our NaN's\n",
    "(google_philly == 0).any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning our NaN's to 0's\n",
    "google_philly.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for NaN's after our transformation\n",
    "google_philly.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://stackoverflow.com/questions/55679401/remove-prefix-or-suffix-substring-from-column-headers-in-pandas\n",
    "\n",
    "# removing 'symptom:' prefix from all our symptoms\n",
    "google_philly.columns = google_philly.columns.map(\n",
    "    lambda x: x.removeprefix('symptom:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_philly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "google_philly.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public COVID-19 PA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start processing the public_data - make the `Date` the index\n",
    "public_data['Date'] = pd.to_datetime(public_data['Date'])\n",
    "public_data.set_index('Date', inplace=True)\n",
    "public_data = public_data.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the data\n",
    "public_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting the data to only include Philadelphia\n",
    "public_philly = public_data[public_data.Jurisdiction == 'Philadelphia']\n",
    "# making it so the dates will match when we join the two dataframes\n",
    "public_philly = public_philly.loc[: \"2022-11-16\"]\n",
    "# looking at the data again\n",
    "public_philly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no NaN's that we have to deal with\n",
    "public_philly.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the Google and Public COVID-19 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "google_philly = google_philly.loc[\"2020-03-01\": \"2022-11-16\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying both the `New Cases` and the `7-day Average New Cases` as the target variable, the `7-day Average New Cases` was chosen - and after running both models, it had a lower error rate according to all metrics (RMSE, MAE, MAPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join data sets and decide on target variable -drop the first 7 days with missing data\n",
    "philly_data = pd.concat(\n",
    "    [google_philly, public_philly['7-day Average New Cases']], axis=1).dropna()\n",
    "# '7-day Average New Cases' is too long - I renamed it as 'Target' instead\n",
    "philly_data.rename(columns={'7-day Average New Cases': 'Target'}, inplace=True)\n",
    "target = philly_data['Target']\n",
    "symptoms = philly_data.drop(['Target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "philly_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "philly_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# looking at the distribution of our target variable\n",
    "target.hist(bins=100, figsize=(16, 8), alpha=0.5,\n",
    "            edgecolor=\"black\", linewidth=2)\n",
    "plt.title(\n",
    "    '7-Day Average COVID Cases in Philadelphia ', fontsize=20)\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number of Confimed Cases', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Train Split: Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will leave us with 20 days in our test set and 961 in our training set\n",
    "cutoff_test = 20\n",
    "\n",
    "X_train = symptoms[:-cutoff_test]\n",
    "X_test = symptoms[-cutoff_test: ]\n",
    "\n",
    "y_train = target[:-cutoff_test]\n",
    "y_test = target[-cutoff_test:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the raw data\n",
    "ax = public_philly['New Cases'].plot(figsize=(20, 10), linewidth=3)\n",
    "plt.title('New Daily COVID-19 Cases in Philadelphia', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at our target variable\n",
    "ax = target.plot(figsize=(20, 10), linewidth=3)\n",
    "plt.title('7-day Average New COVID-19 Cases in Philadelphia', fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model of the Target Variable\n",
    "Now that we've seen the target variable, lets check for stationarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicky Fuller Test for Stationarity\n",
    "def dicky_fuller(TS):\n",
    "    result = adfuller(TS)\n",
    "    print('ADF Statistic: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicky Fuller Test for Stationarity\n",
    "dicky_fuller(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target data is stationary! Lets do a grid search for the optimal orders for our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origionally all of the max elements were set to 5, but that took 2+ hours to run, so I lowered it to 3\n",
    "def sarima_elements(TS):\n",
    "    '''\n",
    "This function will calculate the optimal elements to use in our model. The model we will be using following \n",
    "this function requires both the time series and the order of elements in order to create the model. \n",
    "\n",
    "An ARIMA model requires 3 elements - p, d, and q. This function will cycle through all numbers between 0-2 \n",
    "to find the optimal values for these elements.\n",
    "\n",
    "p - this is the order of the auto-regressive (AR) part of the ARIMA model. This states the lag we want to \n",
    "incorporate into our model, thereby enabling our model to take the past into consideration. \n",
    "d - integrated (I) part of the model, which states the order of \n",
    "how many times the model has been differenced to find optimal  stationarity.\n",
    "q - this is the moving average (MA) order of the ARIMA model, which represents the error found in the model. \n",
    "\n",
    "The seasonal ARIMA contains these three elements along with a seasonal element, set here as 52, to represent \n",
    "the approximate number of weeks in a year.  \n",
    "    '''\n",
    "    auto = pm.auto_arima(TS, start_p=4, start_q=0, max_p=7,\n",
    "                         max_q=3, max_d=3, start_P=0, start_Q=0, max_P=3,\n",
    "                         max_Q=3, m=52, max_order=None, stepwise=True,\n",
    "                         trace=True, random_state=42)\n",
    "    return auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "% % time\n",
    "target_elements = sarima_elements(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the plots and summary about our model\n",
    "target_elements.plot_diagnostics(figsize=(12, 8))\n",
    "target_elements.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the first and last date of the test data\n",
    "last_date_test = y_test.last_valid_index()\n",
    "first_date_test = y_test.first_valid_index()\n",
    "print(f\" First Date: {first_date_test}, Last Date: {last_date_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our baseline ARIMA model for our target data\n",
    "ARIMA_MODEL = sm.tsa.statespace.SARIMAX((y_train), order=target_elements.order,\n",
    "                                        seasonal_order=target_elements.seasonal_order, enforce_stationarity=False,\n",
    "                                        enforce_invertibility=False)\n",
    "\n",
    "# Fit the model and return the results\n",
    "output = ARIMA_MODEL.fit()\n",
    "\n",
    "# forcast\n",
    "output_forcast = output.forecast(steps=last_date_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the confidence interval for our model\n",
    "output_conf = output.get_forecast(steps=last_date_test).conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(y_train, y_test, predicted_forcast, title, y_lable, conf_interval=None, cutoff_date=None):\n",
    "    \"\"\" \n",
    "    The purpose of this function is to plot both the real and forcasted data for timeseries data. \n",
    "    It takes in the training data, the testing data, and the forcasted data. Both the title and y_lable\n",
    "    must be specified, and a confidence interval and cutoff date are optional parameters. \n",
    "\n",
    "    \"\"\"\n",
    "    # plotting the prediction\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    # taking only a potion of the data to better show how the predicted data preformed\n",
    "    ax.plot(y_train[cutoff_date:], label='Training Data')\n",
    "    ax.plot(y_test, label='Testing Data')\n",
    "    predicted_forcast.plot(ax=ax, label='Forecasted Data')\n",
    "    # plotting the confidence interval\n",
    "    ax.fill_between(conf_interval.index,\n",
    "                    conf_interval.iloc[:, 0],\n",
    "                    conf_interval.iloc[:, 1], color='k', alpha=0.25)\n",
    "    ax.set_title(title, fontsize=30)\n",
    "    ax.set_xlabel('Dates')\n",
    "    ax.set_ylabel(y_lable)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(y_train, y_test, output_forcast, \"7-Day Average COVID-19 Cases Forcast\",\n",
    "           '7-Day Average COVID-19 Cases', conf_interval=output_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(y_train, y_test, output_forcast, \"7-Day Average COVID-19 Cases Forcast\",\n",
    "           '7-Day Average COVID-19 Cases', conf_interval=output_conf, cutoff_date='2022-09-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially when this data was modeled, the model predicted a straight line. This led to the assumption that something was wrong with the model. We tried rerunning the parameters, subtracting the moving average from the target variable, and taking the log of the target data, but none of these actions significantly improved upon the model. The data was also run through Facebook Prophet in a different notebook (see [here](https://github.com/sanderlin2013/Predicting-COVID-19-in-PA/blob/main/alt-notebook_prophet.ipynb) and we tried modeling the data pre and post the Omicron spike seen around January 2022 in another notebook as well (see [here](https://github.com/sanderlin2013/Predicting-COVID-19-in-PA/blob/main/alt_notebook_remove_omicron.ipynb). \n",
    "\n",
    "At the end of the day, none of these methods made a difference - the best model I could find used the 7 day average case counts, and the predictions more or less trended towards a straight line during their prediction. This apparently is not a unique occurrence in univariate SARIMA models - in the official [documentation on forecasting in statsmodels](https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_forecasting.html#Prediction-vs-Forecasting) it states, specifically about a straight line SARIMAX forcast: \"The forecast above may not look very impressive, as it is almost a **straight line**. This is because this is a very simple, univariate forecasting model. Nonetheless, *keep in mind that these simple forecasting models can be extremely competitive*.\"\n",
    "\n",
    "For more information on this phenomenon, feel free to read more [here](https://github.com/statsmodels/statsmodels/issues/3852). \n",
    "\n",
    "Finally, it became clear that what was most affecting my predictions was that the model was trying to predict too far into the future. By reducing the test data from 20% of the dataset (almost 100 days) to 20 days, the model performance vastly improved. \n",
    "\n",
    "Lets look at some loss functions to assess how well the model preformed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/3308102/how-to-extract-the-n-th-elements-from-a-list-of-tuples\n",
    "forcast_date_list = list(output_forcast.items())\n",
    "n = 1\n",
    "forcast_list = [x[n] for x in forcast_date_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE:', np.mean(abs(forcast_list - y_test.values)))\n",
    "print('RMSE:', np.sqrt(np.mean((forcast_list - y_test.values)**2)))\n",
    "print('MAPE:', np.mean(abs((forcast_list - y_test.values)/y_test.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these error terms and looking at the plot of our model, we can see that our model isn't perfect, and like most time series performs worse (the confidence interval widens) the farther out we try to predict the data. \n",
    "\n",
    "Lets see if we can improve on these predictions by using a **multivariate time series model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Time Series Model\n",
    "\n",
    "## PCA\n",
    "\n",
    "The first step to trying out multivariate models is figuring out what variables we would like to include in our model! In our dataset we have over 400 symptoms to consider from the Google dataset - using all of them would be too computationally expensive. Luckily, we can use Principal Component Analysis (PCA) for dimensionality reduction. \n",
    "\n",
    "PCA involves transforming a large set of variables into a smaller set of uncorrelated variables while keeping most of the information in the original data. PCA works by identifying the principal components (PCs) of the data, which are linear combinations of the original variables that explain the most variance in the data. The first PC explains the most variance, followed by the second PC, and so on. By selecting only the top few PCs, we can reduce the dimensionality of the data while retaining most of the important information.\n",
    "\n",
    "The first step in using PCA is figuring out how many components we want the PCA to sort our data into. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform PCA on training data\n",
    "pca = PCA(n_components=15).fit(X_train)\n",
    "# plot scree plot\n",
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "plt.plot(PC_values, pca.explained_variance_ratio_,\n",
    "         'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of components we're going to choose will be 2, based on where the scree plot drops off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructuring our `y_train` data so we can add it to our VAR model later\n",
    "df_y_train = pd.DataFrame(y_train)\n",
    "df_y_train = df_y_train.reset_index(names=['date'])\n",
    "df_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the PCA\n",
    "pca = PCA(n_components=2)\n",
    "# fitting and transforming the PCA on our training data\n",
    "principalComponents = pca.fit_transform(X_train)\n",
    "# dataframe with PC's\n",
    "df_pca = pd.DataFrame(principalComponents, columns=['PC1', 'PC2'])\n",
    "\n",
    "# creating on dataframe, with the dates as an index\n",
    "principalComponents = pd.concat([df_y_train, df_pca], axis=1)\n",
    "principalComponents.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# modeling the PC's and target data\n",
    "fig, axes = plt.subplots(nrows=3, figsize=(10, 6))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    data = principalComponents[principalComponents.columns[i]]\n",
    "    ax.plot(data, linewidth=1)\n",
    "    ax.set_title(principalComponents.columns[i])\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# explained variance for each PC\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's see if these variables are stationary by looking at their Dicky-Fuller scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in principalComponents:\n",
    "    print(col)\n",
    "    dicky_fuller(principalComponents[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's further explore these variables using a Granger causality test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Granger Causality Test\n",
    "\n",
    "What is a Granger causality test? Granger causality is a statistical test that helps determine whether one time series can be used to predict another time series. The test is based on the idea that if time series X \"Granger-causes\" another time series Y, then information about the past values of X should help predict the future values of Y better than just using information about the past values of Y alone. In other words, if changes in X can be used to predict changes in Y better than simply looking at past values of Y, then X is said to Granger-cause Y.\n",
    "\n",
    "Here we are going to look at the Granger-cause between our target variable and PC1. When looking at these Granger-causes, it's most important to look at the p-values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('7-day Average New Cases causes PC1\\n')\n",
    "print('------------------')\n",
    "granger_1 = grangercausalitytests(principalComponents[['PC1', 'Target']], 10)\n",
    "\n",
    "print('\\nPC1 causes 7-day Average New Cases\\n')\n",
    "print('------------------')\n",
    "granger_2 = grangercausalitytests(principalComponents[['Target', 'PC1']], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that while there are only a few viable lags when examining if our target variable Granger-causes PC1 (lags 8, 9, and 10), there are more lags that express that PC1 Granger-causes our target variable (lags 8, 9, and 10 as well as 7, 2, and 1). \n",
    "\n",
    "In short, we may be able to use the lags of the other variable to better model our predictions then if we modeled the variables separately. Because we have more lags that express that PC1 Granger-causes our target variable, which seems to indicate a higher likelihood that the causality is more weighed in that direction. \n",
    "\n",
    "I chose not to run a Granger-causality test on PC2, as most of the explained variance (due to the nature of PCA) is in PC1. As such, it is highly likely we'll see non-significant Granger-causality between PC2 and our target variable. \n",
    "\n",
    "That being said we will still include PC2 in our final model as it's interaction terms could aid in making our model more predictive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe with the variables we want in our VAR model\n",
    "var_df = principalComponents[['Target', 'PC1', 'PC2']]\n",
    "var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the model\n",
    "var_model = VAR(var_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code inspiration taken from https://github.com/nachi-hebbar/Multivariate-Time-Series-Forecasting\n",
    "\n",
    "# finding the best order for our VAR model\n",
    "sorted_order = var_model.select_order(maxlags=20)\n",
    "print(sorted_order.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fitting our VAR model\n",
    "var_model = VARMAX(var_df, order=(8, 0), enforce_stationarity=True)\n",
    "fitted_model = var_model.fit(disp=False)\n",
    "print(fitted_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting predictions based on the VAR model\n",
    "predict = fitted_model.get_prediction(start=first_date_test, end=last_date_test)\n",
    "predictions = predict.predicted_mean\n",
    "\n",
    "# get confidence interval\n",
    "var_conf_all = fitted_model.get_forecast(steps=last_date_test).conf_int()\n",
    "var_conf = var_conf_all[[\"lower Target\", \"upper Target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make DataFrame for predictions\n",
    "predictions.columns = ['Target predicted', 'PC1 predicted', 'PC2 predicted', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(var_df[\"Target\"], y_test, predictions[\"Target predicted\"], \"VAR Model COVID-19 Cases Forcast\",\n",
    "           '7-Day Average COVID-19 Cases', conf_interval=var_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(var_df[\"Target\"], y_test, predictions[\"Target predicted\"], \"VAR Model COVID-19 Cases Forcast\",\n",
    "           '7-Day Average COVID-19 Cases', conf_interval=var_conf, cutoff_date='2022-09-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the loss functions to evaluate how our data preformed\n",
    "print('MAE:', np.mean(abs(predictions[\"Target predicted\"] - y_test.values)))\n",
    "print('RMSE:', np.sqrt(\n",
    "    np.mean((predictions[\"Target predicted\"] - y_test.values)**2)))\n",
    "print('MAPE:', np.mean(\n",
    "    abs((predictions[\"Target predicted\"] - y_test.values)/y_test.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above loss functions for our VAR model performed worse compared to the original (base) model. Just a reminder, the original models loss functions were:\n",
    " - MAE: 22.082660363074023\n",
    " - RMSE: 28.91175740690455\n",
    " - MAPE: 0.12197275172442486 \n",
    "\n",
    "As such, I would say that our VAR model (using the VAR model and PCA the way we did) *did not* have better prediction capabilities than simply modeling the daily COVID-19 cases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Summary of Analysis \n",
    "The analysis began by cleaning and processing the Google COVID-19 search data and the public Pennsylvania COVID-19 data. Both datasets were then subset so as to only include Philadelphia county. These datasets were then joined together. After joining the datasets and creating some initial visualizations of the case counts data, a train-test split was performed. We then used [`pmdarima.arima.auto_arima`](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html) to run a grid search. This grid search allowed us to find the optimal orders to model the chosen target variable (`7-Day Average COVID-19 Cases`) using [`statsmodels.tsa.statespace.sarimax.SARIMAX`](https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html#statsmodels.tsa.statespace.sarimax.SARIMAX). This SARIMAX model was our baseline model. We then performed a PCA (Principal Component Analysis), and after assessing the principal components, we used them along with our target variable in our VAR (Vector Auto Regression) model. The VAR model we implemented used [`statsmodels.tsa.statespace.varmax.VARMAX`](https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.varmax.VARMAX.html). We used loss functions to evaluate and compare our two models. The baseline model rounded results were MAE: `22.08`, RMSE: `28.91`, MAPE: `0.12`. The VAR model rounded results were MAE: `69.33`, RMSE: `78.06`, MAPE: `0.39`. As the baseline model outperformed the VAR model, we cannot say that using the Google search trends is helpful in predicting COVID-19 cases, at least with the model created in this notebook. \n",
    "\n",
    "## Recommendations\n",
    "\n",
    "For now, the best way to predict COVID-19 cases in Philadelphia is by looking at Philadelphia's previous COVID-19 cases.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "There are other types of models that may better utilize the COVID-19 Google search data. Trying out these alternate methods were not possible in the time frame allowed for this project, but may give different results. \n",
    "\n",
    "Some possible directions to explore: \n",
    "- Modeling VARMA or VARMAX models.\n",
    "- Using crossvalidation and or recursive modeling methods (documentation [here](https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_forecasting.html#Cross-validation)).\n",
    "- Using neural networks (e.g. [LSTM](https://towardsdatascience.com/multivariate-time-series-forecasting-with-deep-learning-3e7b3e2d2bcf)) in modeling the data. \n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "282.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
